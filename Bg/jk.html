<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <title>Mochi Robot Controller</title>
    <!-- Google MediaPipe Libraries -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/vision_bundle.js" crossorigin="anonymous"></script>
    <link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@400;700&display=swap" rel="stylesheet">
    
    <style>
        body {
            background-color: #0a0a1a;
            color: #00f0ff;
            font-family: 'Orbitron', sans-serif;
            margin: 0;
            display: flex;
            flex-direction: column;
            align-items: center;
            height: 100vh;
            overflow: hidden;
        }
        h2 { margin: 10px 0; text-shadow: 0 0 10px #00f0ff; }
        
        /* Video Container */
        #container {
            position: relative;
            width: 100%;
            max-width: 480px;
            aspect-ratio: 3/4;
            background: #111;
            border: 2px solid #00f0ff;
            border-radius: 15px;
            overflow: hidden;
            box-shadow: 0 0 20px rgba(0, 240, 255, 0.3);
        }
        
        video {
            position: absolute;
            width: 100%;
            height: 100%;
            object-fit: cover;
            transform: scaleX(-1); /* Mirror effect */
        }
        
        canvas {
            position: absolute;
            width: 100%;
            height: 100%;
            transform: scaleX(-1);
        }

        /* Controls */
        .controls {
            margin-top: 15px;
            width: 90%;
            max-width: 400px;
            text-align: center;
        }

        input {
            background: #12122b;
            border: 1px solid #00f0ff;
            color: white;
            padding: 10px;
            width: 70%;
            border-radius: 5px;
            text-align: center;
            font-family: sans-serif;
        }

        button {
            background: #00f0ff;
            color: black;
            border: none;
            padding: 10px 20px;
            border-radius: 5px;
            font-weight: bold;
            cursor: pointer;
            margin-top: 10px;
            width: 100%;
        }

        #status {
            margin-top: 10px;
            font-size: 1.2rem;
            font-weight: bold;
            color: #ff00ff;
        }
    </style>
</head>
<body>

    <h2>MOCHI AI EYE</h2>
    
    <div id="container">
        <video id="webcam" autoplay playsinline></video>
        <canvas id="output_canvas"></canvas>
    </div>

    <div class="controls">
        <input type="text" id="robotIp" placeholder="Enter Robot IP (e.g., 192.168.1.5)">
        <button id="startBtn">START CAMERA</button>
        <div id="status">Waiting...</div>
    </div>

    <script type="module">
        import { HandLandmarker, FilesetResolver } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0";

        const video = document.getElementById("webcam");
        const canvasElement = document.getElementById("output_canvas");
        const canvasCtx = canvasElement.getContext("2d");
        const startBtn = document.getElementById("startBtn");
        const statusText = document.getElementById("status");
        const ipInput = document.getElementById("robotIp");

        let handLandmarker = undefined;
        let lastTriggerTime = 0;
        let isSending = false;

        // Load Saved IP
        if(localStorage.getItem("robotIP")) {
            ipInput.value = localStorage.getItem("robotIP");
        }

        // 1. Initialize MediaPipe AI
        async function createHandLandmarker() {
            statusText.innerText = "Loading AI Model...";
            const vision = await FilesetResolver.forVisionTasks("https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm");
            
            handLandmarker = await HandLandmarker.createFromOptions(vision, {
                baseOptions: {
                    modelAssetPath: "https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task",
                    delegate: "GPU"
                },
                runningMode: "VIDEO",
                numHands: 2
            });
            statusText.innerText = "System Ready! Enter IP & Start.";
            startBtn.disabled = false;
        }
        createHandLandmarker();

        // 2. Start Camera
        startBtn.addEventListener("click", async () => {
            if (!handLandmarker) return;
            
            // Save IP
            localStorage.setItem("robotIP", ipInput.value);

            // Access Mobile Camera (Front)
            const constraints = { video: { facingMode: "user", width: 640, height: 480 } };
            
            try {
                const stream = await navigator.mediaDevices.getUserMedia(constraints);
                video.srcObject = stream;
                video.addEventListener("loadeddata", predictWebcam);
                startBtn.style.display = "none";
                statusText.innerText = "Looking for Gestures...";
            } catch (err) {
                alert("Camera permission denied!");
            }
        });

        // 3. AI Prediction Loop
        async function predictWebcam() {
            // Resize Canvas to match Video
            canvasElement.width = video.videoWidth;
            canvasElement.height = video.videoHeight;
            let startTimeMs = performance.now();

            if (handLandmarker) {
                const results = handLandmarker.detectForVideo(video, startTimeMs);

                canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);

                if (results.landmarks && results.landmarks.length > 0) {
                    let gestureName = "";
                    let detected = false;

                    // Draw Skeleton
                    for (const landmarks of results.landmarks) {
                        drawHand(landmarks);
                    }

                    // --- LOGIC: HEART (2 Hands) ---
                    if (results.landmarks.length === 2) {
                        const h1 = results.landmarks[0];
                        const h2 = results.landmarks[1];
                        
                        // Distance between Index Tips (Point 8)
                        const dIndex = Math.hypot(h1[8].x - h2[8].x, h1[8].y - h2[8].y);
                        // Distance between Thumb Tips (Point 4)
                        const dThumb = Math.hypot(h1[4].x - h2[4].x, h1[4].y - h2[4].y);

                        if (dIndex < 0.05 && dThumb < 0.05) { // 0.05 is threshold
                            gestureName = "HEART â¤ï¸";
                            detected = true;
                        }
                    }

                    // --- LOGIC: ILY (1 Hand) ---
                    if (!detected) {
                        for (const lm of results.landmarks) {
                            // Points: 4(Thumb), 8(Index), 12(Mid), 16(Ring), 20(Pinky)
                            // Y-axis: Lower value is HIGHER on screen
                            
                            const isIndexUp = lm[8].y < lm[6].y;
                            const isPinkyUp = lm[20].y < lm[18].y;
                            const isMidDown = lm[12].y > lm[10].y;
                            const isRingDown = lm[16].y > lm[14].y;

                            if (isIndexUp && isPinkyUp && isMidDown && isRingDown) {
                                gestureName = "ILY SIGN ðŸ¤Ÿ";
                                detected = true;
                            }
                        }
                    }

                    // Trigger Robot
                    if (detected) {
                        statusText.innerText = gestureName;
                        statusText.style.color = "#00ff00";
                        const now = Date.now();
                        
                        // Cooldown 5 seconds
                        if (now - lastTriggerTime > 5000 && !isSending) {
                            triggerRobot();
                            lastTriggerTime = now;
                        }
                    } else {
                        statusText.innerText = "Watching...";
                        statusText.style.color = "#00f0ff";
                    }
                }
            }
            requestAnimationFrame(predictWebcam);
        }

        // Draw Helper
        function drawHand(landmarks) {
            canvasCtx.fillStyle = "#00f0ff";
            canvasCtx.lineWidth = 2;
            canvasCtx.strokeStyle = "#ff00ff";
            
            // Simple Point Drawing
            for (let point of landmarks) {
                const x = point.x * canvasElement.width;
                const y = point.y * canvasElement.height;
                canvasCtx.beginPath();
                canvasCtx.arc(x, y, 4, 0, 2 * Math.PI);
                canvasCtx.fill();
            }
        }

        // 4. Send Signal to Robot
        function triggerRobot() {
            const ip = ipInput.value;
            if (ip.length < 7) { alert("Enter IP First!"); return; }

            isSending = true;
            statusText.innerText = "SENDING SIGNAL...";
            
            // Using fetch with 'no-cors' mode might be needed for simple ESP32 servers
            // But we added headers in ESP32 so standard fetch should work.
            fetch(`http://${ip}/api/love`)
                .then(() => {
                    statusText.innerText = "SENT SUCCESS! â¤ï¸";
                    setTimeout(() => isSending = false, 2000);
                })
                .catch(err => {
                    console.error(err);
                    statusText.innerText = "ERROR CONNECTING";
                    isSending = false;
                });
        }
    </script>
</body>
</html>
